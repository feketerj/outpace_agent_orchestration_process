Claude:


---

{
  "prompts": [
    {
      "role": "Strategist/Architect Agent",
      "category": "Initial Planning",
      "sequence": "1",
      "prompt": "Develop a comprehensive application architecture plan with safe incremental phases, including testing/validation checkpoints and agent communication protocols. Focus on modular build approach. Return only high-level concepts without implementation details.",
      "user_expectation": "Agent provides conceptual roadmap with clear phases, risk mitigation, and handoff points"
    },
    {
      "role": "Strategist/Architect Agent",
      "category": "Planning Validation",
      "sequence": "2",
      "prompt": "Review the session plan and evaluate: communication checkpoints, debugging/testing phases, continuity documentation, and crash recovery capability. Update if needed. Return only when >95% confident in plan viability.",
      "user_expectation": "Agent validates plan completeness and adjusts for maximum success probability"
    },
    {
      "role": "Strategist/Architect Agent",
      "category": "Task List Generation",
      "sequence": "3",
      "prompt": "Create ultra-atomic task list for next build phase. Write for agent consumption, not human readability. Apply safe chunking strategy. Each task must fit within single context window. Include explicit stop points to prevent agent drift.",
      "user_expectation": "Machine-optimized task list preventing scope creep and ensuring deterministic execution"
    },
    {
      "role": "Strategist/Architect Agent",
      "category": "Task List Validation",
      "sequence": "4",
      "prompt": "Audit task list for: gaps, unnecessary complexity, failure points, agent drift potential. Verify 95% confidence that execution agent will succeed without rework. Update and return revised list.",
      "user_expectation": "Refined task list with identified risks mitigated",
      "note": "Source repeatedly emphasizes 95% confidence threshold but doesn't define measurement criteria"
    },
    {
      "role": "Strategist/Architect Agent",
      "category": "Pre-Execution Assessment",
      "sequence": "5",
      "prompt": "Evaluate before execution: sufficient information available? Appropriate task chunking? Adequate context window remaining? Task list restrictiveness balance? Crash recovery feasibility? >95% confidence in meeting objectives? Return assessment only, no code.",
      "user_expectation": "Go/no-go decision with specific adjustments if needed"
    },
    {
      "role": "Execution Agent",
      "category": "Code Generation",
      "sequence": "6",
      "prompt": "Generate code balancing single-pass limitations, safety, and efficiency. Create internal developer rubric, achieve maximum score before execution. Do not reveal rubric. Execute only best-scoring approach.",
      "user_expectation": "Self-validated, optimized code implementation"
    },
    {
      "role": "Execution Agent",
      "category": "Incremental Execution",
      "sequence": "7",
      "prompt": "Execute next task series applying highest success probability concepts. Minimize disruption risk. Stop at logical points. Crosscheck with other agents if appropriate. Return status after completion. Remember: slow is smooth, smooth is fast.",
      "user_expectation": "Measured progress with natural breakpoints and status updates"
    },
    {
      "role": "Execution Agent",
      "category": "Repository Management",
      "sequence": "8",
      "prompt": "Clean repository: identify zero-value files, assess future use likelihood, delete if both are very low. Preserve progress, avoid breaking dependencies. Maintain organized structure for agent operations.",
      "user_expectation": "Streamlined repository optimized for agent token efficiency",
      "note": "Source doesn't specify file categorization criteria or backup requirements"
    },
    {
      "role": "QC/Validation Agent",
      "category": "Code Review",
      "sequence": "9",
      "prompt": "Scan and debug recent code. Balance context limitations with thoroughness. Protect working components. Authorize multiple debugging sweeps. Report issues ranked by fix difficulty. Do not implement corrections.",
      "user_expectation": "Prioritized bug list without breaking existing functionality"
    },
    {
      "role": "QC/Validation Agent",
      "category": "Module Testing",
      "sequence": "10",
      "prompt": "Execute module-by-module test and validation. Proceed slowly, correctly, without breaking components. Multiple passes authorized. Create test folder with event logs. Verify actual vs claimed functionality.",
      "user_expectation": "Comprehensive test results with documented evidence"
    },
    {
      "role": "QC/Validation Agent",
      "category": "Verification",
      "sequence": "11",
      "prompt": "Verify coding agent's claims are 100% factual. Check implementation matches stated functionality. Confirm all fixes successfully implemented. Return only when >95% confident code performs as intended.",
      "user_expectation": "Independent verification of execution agent's work"
    },
    {
      "role": "Documentation Agent",
      "category": "Continuity Documentation",
      "sequence": "12",
      "prompt": "Update agent communication/continuity documents with: current status, completed work, lessons learned, session number, next steps. Optimize for machine reading. Return document names and specific lines for next agent.",
      "user_expectation": "Machine-readable handoff enabling seamless continuation",
      "note": "Source emphasizes machine optimization but doesn't specify format (JSON, YAML, etc.)"
    },
    {
      "role": "Documentation Agent",
      "category": "Lessons Learned",
      "sequence": "13",
      "prompt": "Create lessons learned document: iteration process, discoveries, successes, failures, improvement suggestions, unnecessary/salient steps. Include tags and metadata. Be ruthlessly objective. Goal: data mine for future workflow optimization.",
      "user_expectation": "Actionable insights for process improvement"
    },
    {
      "role": "Documentation Agent",
      "category": "Progress Reports",
      "sequence": "14",
      "prompt": "Update progress reports (start new if >750 lines). Include session number, metadata, agent-optimized formatting. Document progress, bugs, fixes, test results for auditing. Write for agent consumption.",
      "user_expectation": "Comprehensive audit trail in machine-parseable format"
    },
    {
      "role": "Documentation Agent",
      "category": "Version Control",
      "sequence": "15",
      "prompt": "Push changes to GitHub/repository. Ensure progress synchronized for catastrophic loss recovery. Execute periodically per established protocol.",
      "user_expectation": "Consistent backup points for disaster recovery"
    },
    {
      "role": "Any Agent",
      "category": "Recommendation Request",
      "sequence": "As needed",
      "prompt": "Provide focused recommendations tailored to user preferences and experience level. Generate only most impactful items. Avoid scope creep. Adhere to existing build documents/checklists.",
      "user_expectation": "Concise, actionable recommendations within project scope"
    },
    {
      "role": "Any Agent",
      "category": "Alternative Approach",
      "sequence": "As needed",
      "prompt": "Evaluate if more effective methods exist than previously proposed concepts. Focus on conceptual approach only, no code. Consider ROI and risk to existing build.",
      "user_expectation": "Better approach identification without implementation"
    },
    {
      "role": "Any Agent",
      "category": "Quick Approval",
      "sequence": "As needed",
      "prompt": "Continue. / Execute. / Go. / Approved.",
      "user_expectation": "Agent proceeds with planned action immediately",
      "note": "Source shows these as standalone approval commands"
    }
  ]
}


---

Gemini:

| **Initial Plan Development** | Receive a high-level, safe, modular, and incremental plan based on the project objectives. | ```md
Objective: Develop a high-level project plan for a new application based on [Reference Requirements/Context].
Constraints: Adhere strictly to the user philosophy in `user_technical_execution_doc.txt`. The approach must be incremental, modular, and prioritize safety/correctness over speed.
Task: Create a strategic plan that includes:
1. Baseline infrastructure setup.
2. Modular feature development phases.
3. "Make or Buy" analysis for tools/libraries.
4. Checkpoints for Testing and Validation (T&V).
5. Checkpoints for Agent Continuity Documentation updates.
Output: A concise overview of the main phases. Do not provide implementation details or code.
``` |
| **Plan Assessment and Refinement** | Critically review the proposed plan for robustness, risks, and alignment with user philosophy before proceeding. | ```md
Task: Critically evaluate the current project plan [Reference Plan Document].
Focus Areas: Identify risks, gaps, excessive complexity, and adherence to the modular/incremental philosophy. Confirm that T&V and Continuity checkpoints are adequately integrated.
Action: Propose specific updates if deficiencies are found.
Gate Check: Return only when you are >95% confident the plan is robust and ready for the next stage.
``` |
| **Conceptual Approach Generation** | Evaluate the methodology for a specific phase without getting bogged down in code details. | ```md
Objective: Define the approach for the upcoming project phase: [Phase Description].
Constraint: Do not provide code. Focus solely on conceptual approach.
Task: Describe the methodology, required resources, architectural impact, and risk mitigation strategy.
Output: A conceptual strategy analysis.
``` |
| **Determining Next Steps (Dynamic)** | Identify the most logical, impactful, and safest next action when the path forward requires analysis. | ```md
Task: Analyze the current project state by consulting the continuity documentation and repository structure.
Objective: Determine the next sensible step in the project.
Constraints: Minimize risk, strictly avoid scope creep, focus on high ROI actions. Tailor recommendations to the user's non-developer experience level.
Output: A focused recommendation for the immediate next phase of work.
``` |

---

### II. Task List Generation & Refinement

**Role:** Strategist / Advisor / Orchestration Agent
**When to use:** Translating the strategic plan into executable instructions (guardrails) for the Execution Agents.

| Category | User Expectation (When to use) | Evergreen Prompt for Agent (Optimized for "Cut & Paste Contract") |
| :--- | :--- | :--- |
| **Atomic Task List Creation** | Receive an ultra-detailed task list optimized for machine reading, ensuring deterministic execution within context limits. | ```md
Objective: Create a task list for [Project Phase/Module].
Target Audience: AI Coding Execution Agents.
Requirements:
1. Ultra-Atomic Steps: Break down tasks into the smallest executable units. These are strict guardrails.
2. Machine Readability: Optimize the format for agent parsing (e.g., JSON preferred). Write for agents, not the user.
3. Chunking Strategy: Ensure each task block can be completed within the executing agent's context window. More overall tasks are better than backtracking.
4. Control Strategy: Include specific validation checks after critical steps.
Output: The generated task list.
``` |
| **Task List Red Teaming (Rogue Prevention)** | Rigorously review the task list to identify and close gaps that could lead to errors, rework, or agent deviation. | ```md
Task: Act as a Red Team agent and review the generated task list [Reference Task List ID/File].
Objective: Identify weaknesses that could lead to failure or unintended behavior (rogue agents).
Focus Areas:
1. Gaps and Omissions.
2. Unnecessary Code Complexity (distinguish from atomic complexity).
3. Potential Failure Points/Risks to existing functionality.
4. Ambiguity allowing improvisation.
Action: Update the task list to mitigate these risks.
Gate Check: Return when confidence in the task list's robustness exceeds 95%.
``` |
| **The Rubric Method (Internal Self-Assessment)** | Force the agent to internally validate the quality and safety of its plan against best practices before finalization. | ```md
Internal Process Requirement: Before finalizing the plan/task list for [Objective], perform this internal self-assessment. Do not output the rubric or scores to the user.
1. Create a Developer Rubric: Define criteria for success (modularity, safety, efficiency, user philosophy adherence).
2. Evaluate: Score the current draft plan against the rubric.
3. Iterate: Refine the plan until the score is maximized.
Action: Once the maximum score is achieved internally, confirm readiness and output the finalized task list/plan.
``` |

---

### III. Execution Phase

**Role:** Coding Execution Agent
**When to use:** Implementing the code according to the defined atomic task lists.

| Category | User Expectation (When to use) | Evergreen Prompt for Agent (Optimized for "Cut & Paste Contract") |
| :--- | :--- | :--- |
| **Agent Onboarding (Session Start)** | Ensure a new agent understands the exact project status and context before starting work. | ```md
Welcome, new agent. Handoff protocol initiated.
Task: Examine the continuity documentation referenced below:
[Path/Name of Document]
[Lines/JSON Block provided by previous agent]
Action: DO NOT begin coding or making changes.
Output: Provide a concise response confirming your understanding of the current project status and the immediate next task you are assigned to execute.
``` |
| **The Gauntlet (Pre-Execution Gate Check)** | A rigorous assessment of the agent’s readiness, understanding, resources, and capability before authorizing execution. | ```md
STOP. Before executing the task list [Reference Task List ID], perform this assessment. Do not code or make changes.
Answer the following precisely:
1. Information Sufficiency: Do you have all necessary context to execute the task list completely?
2. Context Window Analysis: Do you have sufficient remaining context window to complete all tasks, including reasonably anticipated issues/debugging?
3. Guardrails Assessment: Is the task list sufficient to keep you on target and prevent rework/rogue actions? Is it too restrictive, loose, or just right?
4. Continuity Check: If the system crashes mid-execution, could a new agent accurately assess the completion level and successfully continue the build within 15 minutes?
5. Success Confidence: Do you have >95% confidence that upon completion, the features/fixes will perform exactly as intended and meet the user's objectives without breaking existing functionality?
Output: Your assessment. If the assessment fails any point, halt and report the deficiency.
``` |
| **Execute Task List (Standard Directive)** | The agent executes the defined atomic steps precisely, prioritizing stability and safety. | ```md
Task: Execute the assigned task list: [Reference Task List ID/File], Steps [Start] to [End].
Execution Philosophy:
1. Precision: Follow the steps exactly. Do not deviate or add features unless authorized.
2. Safety First: Slow is smooth and smooth is fast. Slow and right is better than fast and wrong. Minimize risk to the existing build.
3. Logical Stops: Halt execution immediately if an error occurs, a validation fails, or the assigned steps are complete.
4. Status Update: After stopping, report the current status and any errors encountered.
``` |
| **Execute Recommendation (Flexible)** | Implement a previously approved recommendation when a detailed task list is not present, using caution and limited discretion. | ```md
User accepts the recommendation regarding [Specific Feature/Fix].
Task: Implement the recommendation.
Constraints: Generate an appropriate amount of code that balances single-pass limitations, safety, and efficiency. Ensure the implementation is modular and does not jeopardize the build.
Discretion: If a critical flaw in previous work is identified, fix it, provided it doesn’t undo overall progress. Be wary of scope creep.
Post-Execution: Perform immediate validation of the implemented code.
``` |
| **General Approval / Continuation** | Simple commands to move the process forward. | ```md
Approved. Execute.

---

Continue.
``` |

---

### IV. Debugging, Testing, and Quality Control (QC)

**Role:** QC/Ver-Val Agent or Execution Agent
**When to use:** Throughout the build process to ensure code quality, functionality, and stability.

| Category | User Expectation (When to use) | Evergreen Prompt for Agent (Optimized for "Cut & Paste Contract") |
| :--- | :--- | :--- |
| **Post-Execution Debugging Sweep** | Immediate check and fix of the code that was just implemented. | ```md
Task: Scan and debug the code implemented during the last execution phase ([Reference Files/Module]).
Objective: Identify and fix any immediate bugs or errors.
Constraints:
1. Safety: Avoid making changes that risk breaking working parts of the application. Focus only on the recently modified code.
2. Iteration: Multiple debugging passes are authorized to achieve stability.
Output: A report of issues found, fixes applied, and confirmation of stability.
``` |
| **Quality Control Audit (Reporting Only)** | Identify issues without implementing fixes, allowing for strategic planning of the debugging phase. | ```md
Task: Perform a Quality Control audit on the recent work [Reference Module/Phase].
Objective: Detect bugs, weak spots, faulty logic, and deviations from requirements.
Action: DO NOT implement any corrections or changes.
Output: Report the issues found, ranked from the easiest to fix to the most challenging. Update the issue tracker/log.
``` |
| **Debugging Plan Generation** | Create a structured approach to resolve identified issues from the QC Audit. | ```md
Objective: Develop a structured debugging plan for the issues identified in the QC Audit [Reference Audit Report/Log].
Requirements:
1. Prioritization: Sequence the debugging tasks logically.
2. Methodology: Define the approach for isolating and resolving each issue.
3. Safety: Ensure the plan minimizes disruption to working functionality.
Output: The debugging plan, formulated as an atomic task list.
``` |
| **Modular Test and Validation (T&V)** | Thorough, systematic testing of a specific module to ensure full functionality. | ```md
Task: Initiate the Test and Validation (T&V) process for Module: [Module Name].
Objective: Verify the module functions exactly as intended and integrates correctly.
Methodology: Proceed slowly and systematically. Test unit functionality, integration, and edge cases. Minimize disruption.
Authorization: Multiple passes are authorized. Log all results (progress, bugs, fixes, test results) for auditing.
Output: Detailed test results and confirmation of module validation.
``` |
| **Confidence Challenge** | Force the agent to re-verify a solution when the user doubts the results. | ```md
User is not convinced by the previous result/fix. The problem appears persistent.
Task: Re-check the code, the logic, and the test results. Perform a deeper analysis.
Gate Check: Report back only when you are >95% confident in the solution. User will conduct a manual review upon confirmation.
``` |

---

### V. Continuity and Repository Management

**Role:** All Agents
**When to use:** Periodically, at checkpoints, and at the end of sessions to maintain project integrity and organization.

| Category | User Expectation (When to use) | Evergreen Prompt for Agent (Optimized for "Cut & Paste Contract") |
| :--- | :--- | :--- |
| **Continuity Update (Periodic)** | Regular updates to ensure seamless handoff in case of a crash or agent switch. Optimized for machine reading. | ```md
Task: Update the relevant agent handoff and continuity documents.
Objective: Ensure the next agent clearly understands the current status, recent progress, and immediate tasks ahead. Ensure alignment with user priorities and philosophy.
Requirements: Optimize format for machine reading (e.g., JSON summary). Avoid unnecessary materials to save context tokens.
Output: Confirmation of update.
``` |
| **Handoff Protocol (Session End/Outgoing Agent)** | A comprehensive update at the end of a work session, providing precise pointers for the next agent. | ```md
Initiate the Handoff Protocol.
Task: Update all relevant continuity documents (logs, TODO, status, etc.).
Content must include: Current status, developments, significant lessons learned, session number/timestamp, and the precise next steps.
Optimization: Maximize for machine reading.
Output: Return ONLY the path/name of the primary continuity document and the exact lines/JSON block the next agent must read to resume the project.
``` |
| **Repository Synchronization (Version Control)** | Ensure progress is saved externally for disaster recovery. | ```md
Task: Synchronize the local repository with the remote version control system (e.g., GitHub).
Objective: Ensure progress is saved and recoverable in case of catastrophic loss.
Action: Stage relevant changes, create a meaningful commit message summarizing the progress, and push to the designated branch [Branch Name].
``` |
| **Repository Housekeeping (Cleanup)** | Maintain a clean repository and reduce token bloat by removing non-value-added files. | ```md
Task: Sweep the repository for cleanup.
Objective: Maintain an organized repo and reduce token bloat for agent operations.
Criteria: Identify files that are no longer value-added (e.g., old test scripts, temporary files, unused plans). If current value is very low and the likelihood they won't be needed again is very low, delete them.
Safety: Think carefully before deleting. Ensure progress is not lost, and functionality is not broken. Retain all essential documentation (reports, handoffs, status docs, lessons learned).
Output: A list of files deleted and confirmation of status.
``` |
| **Lessons Learned Documentation** | Capture insights objectively to refine future workflows. | ```md
Task: Create or update the lessons learned document for this phase/session.
Objective: Objectively capture data to help define future workflows.
Content: Iteration process, discoveries, what went right, what went wrong, suggestions for improvement, unnecessary/salient steps. Must be ruthlessly objective.
Requirements: Include tags and metadata. Store in the designated lessons learned folder.
``` |
| **Context Refresh** | Ensure the agent has the foundational rules and user philosophy loaded into its active context. | ```md
Task: Reread and familiarize yourself with the user's core philosophy and expectations.
Document: `user_technical_execution_doc.txt`
Output: Confirmation that the context is refreshed.
``` |

---

* Identify roles, categories, and lifecycle sequence from mixed prompts.
* Rewrite into evergreen, vendor-neutral prompts with clear outputs.
* Add user expectations and notes where the source is ambiguous.
* Order prompts across SDLC and agent orchestration.
* Validate relevance to application building and JSON schema compliance.

```json
{
  "prompts": [
    {
      "role": "Agent Orchestrator",
      "category": "Decomposition",
      "sequence": "00-Orchestration",
      "prompt": "Decompose {{goal}} into an agent plan. Output JSON with fields: agents[{name, role, goal, inputs, outputs}], dependencies[{from, to, rationale}], artifacts[{id, type, owner}], stop_conditions, review_gates.",
      "user_expectation": "Receives a machine-runnable multi-agent plan with dependencies and gates.",
      "note": "Reference prompts not provided; plan uses standard SDLC phases."
    },
    {
      "role": "Agent Orchestrator",
      "category": "IO-Contracts",
      "sequence": "00-Orchestration",
      "prompt": "Define strict I/O contracts for each agent. Output JSON Schema per agent: input_schema, output_schema, error_schema, sample_payloads.",
      "user_expectation": "Receives schemas to wire agents without ambiguity."
    },
    {
      "role": "Agent Orchestrator",
      "category": "Sequencing",
      "sequence": "00-Orchestration",
      "prompt": "Map the agent plan to a DAG. Output YAML with nodes, edges, triggers (event|time|gate), retries, timeouts.",
      "user_expectation": "Receives a clear execution graph with triggers."
    },
    {
      "role": "Agent Orchestrator",
      "category": "Governance",
      "sequence": "00-Orchestration",
      "prompt": "Define guardrails. Specify allowed tools, data scopes, PII rules, rate limits, and approval gates. Output a policy file in YAML.",
      "user_expectation": "Receives enforceable policies for safe execution."
    },
    {
      "role": "Agent Orchestrator",
      "category": "Handoffs",
      "sequence": "00-Orchestration",
      "prompt": "Design handoff protocol. Specify artifact addressing, versioning, and context windows. Output a contract with examples.",
      "user_expectation": "Receives deterministic cross-agent handoffs."
    },
    {
      "role": "Agent Orchestrator",
      "category": "Evaluation",
      "sequence": "00-Orchestration",
      "prompt": "Create acceptance checks per agent. Output test cases as JSON: {id, agent, input, expected_output, tolerance, fail_action}.",
      "user_expectation": "Receives verifiable checks to gate progress."
    },
    {
      "role": "Agent Orchestrator",
      "category": "Retrospective",
      "sequence": "00-Orchestration",
      "prompt": "Produce a short retro on orchestration after a run. Output bullets: wins, risks, defects, actions, owner, due_date.",
      "user_expectation": "Receives improvements tied to owners."
    },
    {
      "role": "Product Owner / Strategist",
      "category": "Problem Framing",
      "sequence": "01-Discovery",
      "prompt": "Summarize the problem for {{project_name}}. Output: business_goal, target_users, jobs_to_be_done, constraints, out_of_scope.",
      "user_expectation": "Receives crisp problem and scope."
    },
    {
      "role": "Product Owner / Strategist",
      "category": "User Stories",
      "sequence": "01-Discovery",
      "prompt": "Draft user stories with acceptance criteria for {{target_users}}. Output a table: story_id, as_a, I_want, so_that, acceptance_criteria.",
      "user_expectation": "Receives prioritized stories with AC."
    },
    {
      "role": "Product Owner / Strategist",
      "category": "Success Metrics",
      "sequence": "01-Discovery",
      "prompt": "Define KPIs for {{project_name}}. Output: metric, definition, baseline, target, measurement_method, cadence.",
      "user_expectation": "Receives measurable outcomes."
    },
    {
      "role": "Product Owner / Strategist",
      "category": "Risks and Assumptions",
      "sequence": "01-Discovery",
      "prompt": "List top risks and assumptions. Output a risk register: id, description, impact, likelihood, mitigation, owner.",
      "user_expectation": "Receives a risk register."
    },
    {
      "role": "Product Owner / Strategist",
      "category": "Scope Boundaries",
      "sequence": "01-Discovery",
      "prompt": "Create an in-scope vs out-of-scope matrix for {{release_window}}. Output two lists with rationale.",
      "user_expectation": "Receives scope clarity for delivery."
    },
    {
      "role": "Solution Architect",
      "category": "System Context",
      "sequence": "02-Architecture",
      "prompt": "Produce a C4 Level-1 text spec for {{project_name}}. Output: system_purpose, external_users, external_systems, relationships, key_quality_attributes.",
      "user_expectation": "Receives high-level system context."
    },
    {
      "role": "Solution Architect",
      "category": "ADRs",
      "sequence": "02-Architecture",
      "prompt": "Draft initial ADRs for {{stack_preferences}}. Output ADR stubs: title, status, context, decision, consequences.",
      "user_expectation": "Receives ADR skeletons for key choices."
    },
    {
      "role": "Solution Architect",
      "category": "Service Boundaries",
      "sequence": "02-Architecture",
      "prompt": "Propose service decomposition. Output services: name, responsibility, APIs, data_owned, dependencies, scale_driver.",
      "user_expectation": "Receives a service map."
    },
    {
      "role": "Solution Architect",
      "category": "NFR Tactics",
      "sequence": "02-Architecture",
      "prompt": "Map {{nonfunctional_requirements}} to tactics. Output: nfr, tactic, design_implication, verification_method.",
      "user_expectation": "Receives NFR-to-tactic mapping."
    },
    {
      "role": "Solution Architect",
      "category": "Data Flows",
      "sequence": "02-Architecture",
      "prompt": "Describe key data flows. Output: source, sink, format, frequency, volume, latency_target, error_handling.",
      "user_expectation": "Receives documented data flows."
    },
    {
      "role": "Solution Architect",
      "category": "Security Model",
      "sequence": "02-Architecture",
      "prompt": "Select high-level authn/authz patterns for {{target_users}}. Output: identity_source, session_model, permission_model, secrets_scopes.",
      "user_expectation": "Receives security model options."
    },
    {
      "role": "Full-Stack Scaffolder",
      "category": "Repo Structure",
      "sequence": "03-Scaffolding",
      "prompt": "Propose repo layout for {{mono_or_poly}}. Output: directories, purpose, tooling, code_owners.",
      "user_expectation": "Receives repo scaffold plan."
    },
    {
      "role": "Full-Stack Scaffolder",
      "category": "Standards",
      "sequence": "03-Scaffolding",
      "prompt": "Define language and style standards for {{language}}. Output: linters, formatters, commit_conventions, branch_policy.",
      "user_expectation": "Receives enforceable standards."
    },
    {
      "role": "Full-Stack Scaffolder",
      "category": "Tasks",
      "sequence": "03-Scaffolding",
      "prompt": "Design a Makefile or task runner plan. Output tasks: setup, test, lint, build, run, release.",
      "user_expectation": "Receives common dev tasks."
    },
    {
      "role": "Full-Stack Scaffolder",
      "category": "Local Dev",
      "sequence": "03-Scaffolding",
      "prompt": "Specify local dev environment. Output: runtime_versions, services, env_vars, seeds, startup_steps.",
      "user_expectation": "Receives reproducible local setup."
    },
    {
      "role": "API Contract Agent",
      "category": "Resource Design",
      "sequence": "04-API-Contracts",
      "prompt": "Draft OpenAPI for core resources {{core_resources}} using REST or RPC as appropriate. Include paths, schemas, errors, examples.",
      "user_expectation": "Receives a first-pass API spec.",
      "note": "Return vendor-agnostic OpenAPI 3.1 unless specified."
    },
    {
      "role": "API Contract Agent",
      "category": "Conventions",
      "sequence": "04-API-Contracts",
      "prompt": "Define pagination, filtering, sorting, and search conventions. Output: patterns, query_params, examples, tradeoffs.",
      "user_expectation": "Receives consistent query conventions."
    },
    {
      "role": "API Contract Agent",
      "category": "Errors",
      "sequence": "04-API-Contracts",
      "prompt": "Define an error model. Output JSON: code, http_status, message, details, remediation, correlation_id.",
      "user_expectation": "Receives a uniform error contract."
    },
    {
      "role": "API Contract Agent",
      "category": "Versioning",
      "sequence": "04-API-Contracts",
      "prompt": "Choose API versioning strategy. Output: scheme, compatibility_rules, deprecation_policy, examples.",
      "user_expectation": "Receives versioning rules."
    },
    {
      "role": "API Contract Agent",
      "category": "Mocks",
      "sequence": "04-API-Contracts",
      "prompt": "Plan mock server generation from OpenAPI. Output: tooling options, routes to mock, data strategies.",
      "user_expectation": "Receives a mock server plan."
    },
    {
      "role": "Data Modeler",
      "category": "Conceptual Model",
      "sequence": "05-Data-Model",
      "prompt": "Propose a conceptual ERD. Output: entities, relationships, cardinalities, invariants.",
      "user_expectation": "Receives a clear domain model."
    },
    {
      "role": "Data Modeler",
      "category": "Naming and Migrations",
      "sequence": "05-Data-Model",
      "prompt": "Define naming conventions and migration policy for {{database}}. Output: rules, migration_steps, rollback_guidelines.",
      "user_expectation": "Receives consistent DB rules."
    },
    {
      "role": "Data Modeler",
      "category": "Seed Data",
      "sequence": "05-Data-Model",
      "prompt": "Plan seed data and fixtures. Output: datasets, generation_rules, privacy_notes.",
      "user_expectation": "Receives safe repeatable seeds."
    },
    {
      "role": "Data Modeler",
      "category": "Indexing",
      "sequence": "05-Data-Model",
      "prompt": "Propose indexing strategy for key queries. Output: query_patterns, suggested_indexes, maintenance_costs.",
      "user_expectation": "Receives index recommendations.",
      "note": "Database engine unspecified; provide generic B-Tree and covering index options."
    },
    {
      "role": "Backend Engineer",
      "category": "Service Skeleton",
      "sequence": "06-Backend",
      "prompt": "Generate a service skeleton for {{framework}} with health, readiness, and metrics endpoints. Output file tree and stubs.",
      "user_expectation": "Receives runnable service scaffold."
    },
    {
      "role": "Backend Engineer",
      "category": "Core Endpoints",
      "sequence": "06-Backend",
      "prompt": "Implement CRUD endpoints for {{resource}} per API spec. Include validation, serialization, and errors.",
      "user_expectation": "Receives endpoint stubs aligned to the contract."
    },
    {
      "role": "Backend Engineer",
      "category": "Persistence",
      "sequence": "06-Backend",
      "prompt": "Add repository layer for {{database}}. Output interfaces, transaction strategy, and sample queries.",
      "user_expectation": "Receives persistence layer design."
    },
    {
      "role": "Backend Engineer",
      "category": "Background Jobs",
      "sequence": "06-Backend",
      "prompt": "Define job processing for {{use_case}}. Output: queue, retry_policy, idempotency_key, scheduling.",
      "user_expectation": "Receives reliable job patterns."
    },
    {
      "role": "Backend Engineer",
      "category": "Resilience",
      "sequence": "06-Backend",
      "prompt": "Add resilience patterns. Output: timeouts, retries, backoff, circuit_breakers with defaults.",
      "user_expectation": "Receives fault-tolerant configs."
    },
    {
      "role": "Backend Engineer",
      "category": "Configuration",
      "sequence": "06-Backend",
      "prompt": "Specify configuration strategy. Output: env_vars, config_files, precedence, secrets_boundary.",
      "user_expectation": "Receives clear config boundaries."
    },
    {
      "role": "Frontend Engineer",
      "category": "Component Inventory",
      "sequence": "07-Frontend",
      "prompt": "List screens and components for {{user_flows}}. Output: route, component, state, data_sources.",
      "user_expectation": "Receives component map by route."
    },
    {
      "role": "Frontend Engineer",
      "category": "State Management",
      "sequence": "07-Frontend",
      "prompt": "Propose state strategy. Output: server_state vs client_state, caching rules, invalidation triggers.",
      "user_expectation": "Receives predictable state plan."
    },
    {
      "role": "Frontend Engineer",
      "category": "API Client",
      "sequence": "07-Frontend",
      "prompt": "Generate API client layer from OpenAPI. Include typing, auth, error normalization.",
      "user_expectation": "Receives a typed client layer."
    },
    {
      "role": "Frontend Engineer",
      "category": "Accessibility",
      "sequence": "07-Frontend",
      "prompt": "Set A11y acceptance rules. Output: keyboard_support, labels, color_contrast, focus_management, test_cases.",
      "user_expectation": "Receives enforceable A11y checks."
    },
    {
      "role": "Frontend Engineer",
      "category": "Layouts",
      "sequence": "07-Frontend",
      "prompt": "Define responsive layout grid and spacing scale. Output tokens and examples.",
      "user_expectation": "Receives a layout system."
    },
    {
      "role": "Frontend Engineer",
      "category": "UX States",
      "sequence": "07-Frontend",
      "prompt": "Specify loading, empty, error, and success states for key views. Output patterns and samples.",
      "user_expectation": "Receives consistent UX state patterns."
    },
    {
      "role": "DevOps / Platform",
      "category": "CI Pipeline",
      "sequence": "08-DevOps",
      "prompt": "Design CI pipeline for {{language}}. Stages: lint, test, build, scan, package, cache. Output YAML-like spec.",
      "user_expectation": "Receives a portable CI plan.",
      "note": "CI vendor unspecified; keep steps generic."
    },
    {
      "role": "DevOps / Platform",
      "category": "Containerization",
      "sequence": "08-DevOps",
      "prompt": "Propose container build for {{service}}. Output: base_image, build_args, multi-stage steps, healthcheck.",
      "user_expectation": "Receives a reproducible image plan."
    },
    {
      "role": "DevOps / Platform",
      "category": "IaC",
      "sequence": "08-DevOps",
      "prompt": "Outline Infrastructure as Code for {{cloud}}. Output: environments, state_management, modules, drift_detection.",
      "user_expectation": "Receives IaC structure."
    },
    {
      "role": "DevOps / Platform",
      "category": "Environments",
      "sequence": "08-DevOps",
      "prompt": "Define environment matrix. Output: dev, test, staging, prod with config_diffs, promotion_rules.",
      "user_expectation": "Receives environment definitions."
    },
    {
      "role": "DevOps / Platform",
      "category": "Secrets",
      "sequence": "08-DevOps",
      "prompt": "Specify secrets management. Output: storage, rotation, access_policies, local_dev_substitutes.",
      "user_expectation": "Receives secrets handling rules."
    },
    {
      "role": "DevOps / Platform",
      "category": "Artifacts",
      "sequence": "08-DevOps",
      "prompt": "Plan artifact storage and caching. Output: artifact_types, retention, immutability, cache_keys.",
      "user_expectation": "Receives artifact retention plan."
    },
    {
      "role": "Security Engineer",
      "category": "Threat Model",
      "sequence": "09-Security",
      "prompt": "Threat model {{system_boundary}} using STRIDE. Output: assets, trust_zones, threats, mitigations, residual_risk.",
      "user_expectation": "Receives a prioritized threat model.",
      "note": "Method can be LINDDUN for privacy; defaulting to STRIDE."
    },
    {
      "role": "Security Engineer",
      "category": "AuthN/AuthZ",
      "sequence": "09-Security",
      "prompt": "Design authn/authz for {{audiences}}. Output: flows, token_lifetimes, role_matrix, permission_checks.",
      "user_expectation": "Receives actionable auth design."
    },
    {
      "role": "Security Engineer",
      "category": "Dependency Hygiene",
      "sequence": "09-Security",
      "prompt": "Set dependency and container scanning policy. Output: tools, severity_thresholds, SLA, exceptions_process.",
      "user_expectation": "Receives enforceable scanning policy."
    },
    {
      "role": "Security Engineer",
      "category": "Data Protection",
      "sequence": "09-Security",
      "prompt": "Define data protection. Output: classification, encryption_at_rest, in_transit, key_rotation, data_minimization.",
      "user_expectation": "Receives data security controls."
    },
    {
      "role": "Security Engineer",
      "category": "IR Playbook",
      "sequence": "09-Security",
      "prompt": "Draft incident response playbook. Output: detection, triage, containment, eradication, recovery, postmortem.",
      "user_expectation": "Receives an IR workflow."
    },
    {
      "role": "QA / Test Engineer",
      "category": "Test Strategy",
      "sequence": "10-Testing",
      "prompt": "Create test strategy. Output: levels(unit|integration|e2e), coverage_targets, environments, gating_rules.",
      "user_expectation": "Receives a testing plan."
    },
    {
      "role": "QA / Test Engineer",
      "category": "Unit Tests",
      "sequence": "10-Testing",
      "prompt": "Generate unit test blueprint for {{module}}. Output: cases, inputs, oracles, mocks, edge_conditions.",
      "user_expectation": "Receives unit test cases."
    },
    {
      "role": "QA / Test Engineer",
      "category": "Integration Tests",
      "sequence": "10-Testing",
      "prompt": "Plan integration tests for {{service_interactions}}. Output: contracts, fixtures, mock_strategies, assertions.",
      "user_expectation": "Receives integration test plan."
    },
    {
      "role": "QA / Test Engineer",
      "category": "E2E",
      "sequence": "10-Testing",
      "prompt": "Design end-to-end flows for {{critical_paths}}. Output: sequence, preconditions, steps, expected, cleanup.",
      "user_expectation": "Receives e2e scenarios."
    },
    {
      "role": "QA / Test Engineer",
      "category": "Test Data",
      "sequence": "10-Testing",
      "prompt": "Define test data management. Output: generation_rules, masking, refresh_cadence, ownership.",
      "user_expectation": "Receives safe test data plan."
    },
    {
      "role": "Documentation Agent",
      "category": "README",
      "sequence": "11-Docs",
      "prompt": "Draft README for {{project_name}}. Sections: purpose, architecture_overview, setup, run, test, deploy, troubleshooting.",
      "user_expectation": "Receives a complete README."
    },
    {
      "role": "Documentation Agent",
      "category": "Contributing",
      "sequence": "11-Docs",
      "prompt": "Create CONTRIBUTING guide. Output: branching, commits, code_review_checklist, issue_template, PR_template.",
      "user_expectation": "Receives team-ready contributing docs."
    },
    {
      "role": "Documentation Agent",
      "category": "API Reference",
      "sequence": "11-Docs",
      "prompt": "Generate API reference from OpenAPI. Include examples and curl snippets.",
      "user_expectation": "Receives consumable API docs."
    },
    {
      "role": "Integration Agent",
      "category": "3rd-Party APIs",
      "sequence": "12-Integration",
      "prompt": "Plan integration with {{external_api}}. Output: auth, rate_limits, retries, backoff, circuit_breaker, fallbacks.",
      "user_expectation": "Receives robust integration plan."
    },
    {
      "role": "Integration Agent",
      "category": "Webhooks",
      "sequence": "12-Integration",
      "prompt": "Design webhook endpoints for {{events}}. Output: signatures, idempotency, replay_handling, security.",
      "user_expectation": "Receives secure webhook design."
    },
    {
      "role": "Integration Agent",
      "category": "Idempotency",
      "sequence": "12-Integration",
      "prompt": "Specify idempotency strategy for writes. Output: keys, storage, TTL, conflict_resolution.",
      "user_expectation": "Receives safe write plan."
    },
    {
      "role": "Performance Engineer",
      "category": "SLO/SLA",
      "sequence": "13-Performance",
      "prompt": "Define SLOs for {{critical_paths}}. Output: SLI, target, window, error_budget, alert_thresholds.",
      "user_expectation": "Receives actionable SLOs."
    },
    {
      "role": "Performance Engineer",
      "category": "Load Tests",
      "sequence": "13-Performance",
      "prompt": "Create load test scenarios. Output: workload_model, arrival_rate, datasets, ramp, pass_fail_criteria.",
      "user_expectation": "Receives a load plan."
    },
    {
      "role": "Performance Engineer",
      "category": "Caching",
      "sequence": "13-Performance",
      "prompt": "Propose caching layers for {{hot_paths}}. Output: cache_keys, ttl, invalidation, coherency.",
      "user_expectation": "Receives caching strategy."
    },
    {
      "role": "Observability Agent",
      "category": "Logging",
      "sequence": "14-Observability",
      "prompt": "Define logging schema. Output: levels, fields, correlation_id, PII_policy, sampling.",
      "user_expectation": "Receives structured logging rules."
    },
    {
      "role": "Observability Agent",
      "category": "Metrics",
      "sequence": "14-Observability",
      "prompt": "List service metrics and dashboards. Output: metric, type, labels, alert, dashboard_widget.",
      "user_expectation": "Receives metrics to instrument."
    },
    {
      "role": "Observability Agent",
      "category": "Tracing",
      "sequence": "14-Observability",
      "prompt": "Plan distributed tracing. Output: spans, propagation, sampling, baggage, privacy_notes.",
      "user_expectation": "Receives tracing plan."
    },
    {
      "role": "Compliance & Accessibility Agent",
      "category": "Data Privacy",
      "sequence": "15-Compliance",
      "prompt": "Map data categories to {{compliance_regimes}}. Output: data_type, lawful_basis, retention, DSR_process.",
      "user_expectation": "Receives privacy mapping."
    },
    {
      "role": "Compliance & Accessibility Agent",
      "category": "Accessibility",
      "sequence": "15-Compliance",
      "prompt": "Create A11y compliance checklist for {{ui_stack}}. Output: criteria, test, owner, release_gate.",
      "user_expectation": "Receives executable A11y checks."
    },
    {
      "role": "Compliance & Accessibility Agent",
      "category": "Audit Logs",
      "sequence": "15-Compliance",
      "prompt": "Define audit logging requirements. Output: events, fields, retention, tamper_evidence.",
      "user_expectation": "Receives audit requirements."
    },
    {
      "role": "Release Manager",
      "category": "Versioning",
      "sequence": "16-Release",
      "prompt": "Choose versioning scheme for {{services}}. Output: scheme, release_branches, tagging, backports.",
      "user_expectation": "Receives versioning rules."
    },
    {
      "role": "Release Manager",
      "category": "Release Notes",
      "sequence": "16-Release",
      "prompt": "Template release notes. Sections: highlights, breaking_changes, migrations, fixes, upgrade_steps.",
      "user_expectation": "Receives a reusable template."
    },
    {
      "role": "Operations / Support Agent",
      "category": "Runbooks",
      "sequence": "17-Operations",
      "prompt": "Write runbooks for {{top_incidents}}. Output: symptoms, diagnosis, remediation, rollback, verification.",
      "user_expectation": "Receives actionable runbooks."
    },
    {
      "role": "Operations / Support Agent",
      "category": "On-call",
      "sequence": "17-Operations",
      "prompt": "Define on-call and escalation. Output: schedule, response_SLA, paging_rules, comms_templates.",
      "user_expectation": "Receives an on-call policy."
    },
    {
      "role": "Analytics / Experimentation Agent",
      "category": "Event Taxonomy",
      "sequence": "18-Analytics",
      "prompt": "Create analytics event taxonomy. Output: event_name, properties, owner, PII_flag, QA_checks.",
      "user_expectation": "Receives a clean event spec."
    },
    {
      "role": "Analytics / Experimentation Agent",
      "category": "Experiment Design",
      "sequence": "18-Analytics",
      "prompt": "Design A/B test for {{feature}}. Output: hypothesis, primary_metric, guardrails, sample_size_inputs, exposure, stop_rules.",
      "user_expectation": "Receives an experiment plan."
    },
    {
      "role": "Analytics / Experimentation Agent",
      "category": "Feature Flags",
      "sequence": "18-Analytics",
      "prompt": "Plan feature flag rollout. Output: flag_name, audience, kill_switch, telemetry, cleanup_date.",
      "user_expectation": "Receives safe rollout plan."
    },
    {
      "role": "Internationalization Agent",
      "category": "i18n Plan",
      "sequence": "19-Internationalization",
      "prompt": "Define i18n for {{locales}}. Output: message_format, extraction, pluralization, RTL, date_number_rules.",
      "user_expectation": "Receives i18n implementation plan."
    },
    {
      "role": "Internationalization Agent",
      "category": "Localization Pipeline",
      "sequence": "19-Internationalization",
      "prompt": "Design l10n pipeline. Output: source_of_truth, translation_memory, QA, fallback_rules, release_sync.",
      "user_expectation": "Receives l10n process and rules."
    },
    {
      "role": "LLM/RAG Agent Designer",
      "category": "Capability Spec",
      "sequence": "20-LLM",
      "prompt": "Define LLM capability for {{use_case}}. Output: inputs, outputs, tools, constraints, acceptance_checks.",
      "user_expectation": "Receives a bounded capability spec."
    },
    {
      "role": "LLM/RAG Agent Designer",
      "category": "Retrieval",
      "sequence": "20-LLM",
      "prompt": "Plan retrieval for {{corpus}}. Output: chunking, embeddings_fields, index_layout, freshness, filters.",
      "user_expectation": "Receives a retrieval plan."
    },
    {
      "role": "LLM/RAG Agent Designer",
      "category": "Prompt Templates",
      "sequence": "20-LLM",
      "prompt": "Create prompt templates with slot variables. Output: system, user, tool, examples, negative_prompts.",
      "user_expectation": "Receives reusable prompt pack."
    },
    {
      "role": "LLM/RAG Agent Designer",
      "category": "Evaluation",
      "sequence": "20-LLM",
      "prompt": "Define eval harness. Output: test_set_sources, metrics(exact_match|F1|preference), thresholds, failure_buckets.",
      "user_expectation": "Receives an eval plan.",
      "note": "Metrics vary by task; select minimally sufficient set."
    },
    {
      "role": "LLM/RAG Agent Designer",
      "category": "Safety",
      "sequence": "20-LLM",
      "prompt": "List safety tests. Output: jailbreak_attempts, PII_leakage, hallucination_traps, prompt_injection, escalation_rules.",
      "user_expectation": "Receives safety test cases."
    },
    {
      "role": "Migration / Modernization Agent",
      "category": "Strangler Plan",
      "sequence": "21-Migration",
      "prompt": "Plan strangler migration for {{legacy_system}}. Output: facets, anti_corruption_layers, routing, stepwise_cuts.",
      "user_expectation": "Receives incremental migration steps."
    },
    {
      "role": "Migration / Modernization Agent",
      "category": "Data Migration",
      "sequence": "21-Migration",
      "prompt": "Map data migration. Output: sources, targets, transforms, verification, backfill, rollback.",
      "user_expectation": "Receives safe data migration plan."
    }
  ]
}
```
